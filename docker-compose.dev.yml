services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: app-dev
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    depends_on:
      - db
      - redis
      - rabbitmq
      - celery
      - minio
      - ollama

    environment:
      ENVIRONMENT: development
      SEED_DB_ON_EMPTY: ${SEED_DB_ON_EMPTY:-true}

      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}

      RABBITMQ_HOST: ${RABBITMQ_HOST}
      RABBITMQ_USER: ${RABBITMQ_DEFAULT_USER}
      RABBITMQ_PASSWORD: ${RABBITMQ_DEFAULT_PASS}

      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL:-http://minio:9000}
      S3_PUBLIC_BASE_URL: ${S3_PUBLIC_BASE_URL:-http://localhost:9000}
      S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID:-minioadmin}
      S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY:-minioadmin}
      S3_REGION: ${S3_REGION:-us-east-1}
      S3_BUCKET: ${S3_BUCKET:-memoryful}

      LLM_PROVIDER: ${LLM_PROVIDER:-local}
      OPENAI_TEMPERATURE: ${OPENAI_TEMPERATURE:-0.4}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      LOCAL_LLM_BASE_URL: ${LOCAL_LLM_BASE_URL:-http://ollama:11434/v1}
      LOCAL_LLM_MODEL: ${LOCAL_LLM_MODEL:-llama3.1}
      LOCAL_LLM_API_KEY: ${LOCAL_LLM_API_KEY:-local}
      ANTHROPIC_MODEL: ${ANTHROPIC_MODEL:-claude-3-5-sonnet-20240620}

    command: >
      sh -c "
        alembic upgrade head &&
        uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
      "


  db:
    image: postgres:16
    container_name: db-dev
    restart: always
    ports:
      - "5444:${POSTGRES_PORT}"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data_dev:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 1s
      retries: 5

  redis:
    image: redis:6.0.7
    container_name: redis-dev
    restart: always
    volumes:
      - redis_data_dev:/data
    ports:
      - "${REDIS_PORT}:${REDIS_PORT}"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 1s
      retries: 5

  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: rabbitmq-dev
    restart: always
    ports:
      - "${RABBITMQ_PORT}:${RABBITMQ_PORT}"
      - "${RABBITMQ_WEB_PORT}:${RABBITMQ_WEB_PORT}"
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS}
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 3s
      timeout: 1s
      retries: 5

  celery:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: celery-dev
    volumes:
      - .:/app
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: always
    environment:
      CELERY_BROKER_URL: amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@${RABBITMQ_HOST}:${RABBITMQ_PORT}//
      CELERY_RESULT_BACKEND: redis://${REDIS_HOST}:${REDIS_PORT}/${REDIS_DB}
      ENVIRONMENT: development
      LLM_PROVIDER: ${LLM_PROVIDER:-local}
      LOCAL_LLM_BASE_URL: ${LOCAL_LLM_BASE_URL:-http://ollama:11434/v1}
      LOCAL_LLM_MODEL: ${LOCAL_LLM_MODEL:-llama3.1}
      LOCAL_LLM_API_KEY: ${LOCAL_LLM_API_KEY:-local}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      OPENAI_TEMPERATURE: ${OPENAI_TEMPERATURE:-0.4}
    command: >
      sh -c "
        celery -A app.core.celery_app worker -Q ai_queue --loglevel=debug --hostname=celery@ai_worker &
        celery -A app.core.celery_app worker -Q apps_queue --loglevel=debug --hostname=celery@apps_worker &
        celery -A app.core.celery_app worker -Q email_queue --loglevel=debug --hostname=celery@email_worker &
        celery -A app.core.celery_app worker -Q notifications_queue --loglevel=debug --hostname=celery@notifications_worker &
        celery -A app.core.celery_app worker -Q system_queue --loglevel=debug --hostname=celery@system_worker &
        wait"

  flower:
    image: mher/flower
    container_name: flower-dev
    restart: on-failure
    command: celery --broker=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@${RABBITMQ_HOST}:${RABBITMQ_PORT}// flower
    ports:
      - "${FLOWER_PORT}:${FLOWER_PORT}"
    depends_on:
      - celery

  minio:
    image: minio/minio:latest
    container_name: minio-dev
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY_ID:-minioadmin}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_ACCESS_KEY:-minioadmin}
    volumes:
      - minio_data_dev:/data

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-dev
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data_dev:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  redis_data_dev:
  postgres_data_dev:
  minio_data_dev:
  ollama_data_dev:
